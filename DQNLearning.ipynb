{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from importlib import reload\n",
    "import game2\n",
    "import dqn_agent\n",
    "game2 = reload(game2)\n",
    "dqn_agent = reload(dqn_agent)\n",
    "from game2 import Game\n",
    "from dqn_agent import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "moving_average = lambda x, **kw: pd.DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Flattener(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size, *_ = x.shape\n",
    "        return x.reshape(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, in_channels, out_size, h=210, w=160):\n",
    "      super(Policy, self).__init__()\n",
    "      def conv2d_size_out(size, kernel_size = 4, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "      \n",
    "      self.h = h\n",
    "      self.w = w\n",
    "      \n",
    "      convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w, 8, 4)), 3, 1)\n",
    "      convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h, 8, 4)), 3, 1)\n",
    "      linear_input = convh * convw * 64\n",
    "      self.act = nn.Sequential(nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
    "                               nn.BatchNorm2d(32),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "                               nn.BatchNorm2d(64),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "                               nn.BatchNorm2d(64),\n",
    "                               nn.ReLU(),\n",
    "                               Flattener(),\n",
    "                               nn.Linear(linear_input, 1000),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(1000, out_size)\n",
    "                               )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if len(X.shape) < 4:\n",
    "            X = X.view(1, X.shape[0], X.shape[1], X.shape[2])\n",
    "        return self.act(X)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "       return self.state_dict()\n",
    "    \n",
    "    def set_parameters(self, state_dict):\n",
    "        self.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "in_channels = 3\n",
    "out_size = 3\n",
    "transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                transforms.Resize(size=(84, 84)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.20, 0.20, 0.20])]\n",
    "                               )\n",
    "\n",
    "model = Policy(in_channels, out_size, h=84, w=84).to(device)\n",
    "target_model = Policy(in_channels, out_size, h=84, w=84).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "dqn = DQNAgent(model, target_model, optimizer, loss_func , out_size, memory=30000, n_multi_step=2, device=device,\n",
    "               double_dqn=True, transform=transform, gamma=.99, epsilon_start=1.0, epsilon_end=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Step: 0, rewards: -1, mean loss: 0.000\n",
      "Step: 1, rewards: -1, mean loss: 0.000\n",
      "Step: 2, rewards: -1, mean loss: 0.000\n",
      "Step: 3, rewards: -1, mean loss: 0.000\n",
      "Step: 4, rewards: -1, mean loss: 0.035\n",
      "Step: 5, rewards: 0, mean loss: 0.017\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e5c204dea7b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_train_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1001\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mupdate_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\RL-CartPole-v0\\dqn_agent.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, env, steps, start_train_steps, batch_size, train_every, update_model)\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtrain_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mstart_train_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m                             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\RL-CartPole-v0\\dqn_agent.py\u001b[0m in \u001b[0;36mbatch_create\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         return (np.array(states, dtype=np.float32), np.array(actions, dtype=np.int64),\n\u001b[1;32m--> 137\u001b[1;33m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterminate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "env = Game(False)\n",
    "dqn.fit(env, 500, batch_size=batch_size, train_every=1, start_train_steps=10001,  update_model=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_agent(env, transform=None, agent=None, n_episodes=100):\n",
    "    \"\"\"Runs agent for n_episodes in environment and calclates mean reward.\n",
    "    \n",
    "    Args:\n",
    "        env: The environment for agent to play in\n",
    "        agent: The agent to play with. Defaults to None - \n",
    "            in this case random agent is used.\n",
    "        n_episodes: Number of episodes to play. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    total_reward = []\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('output.avi', fourcc, 25, (500, 400))\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        episode_reward = 0\n",
    "        observation = env.reset()\n",
    "        t = 0\n",
    "        while True:\n",
    "            if agent:\n",
    "                agent.train(False)\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    if transform:\n",
    "                        observation = transform(observation)\n",
    "                    probs = agent(torch.as_tensor(observation, device=device))\n",
    "                    action = torch.argmax(F.softmax(probs, dim=-1)).item()\n",
    "            else:\n",
    "                action = random.randint(0, 2)\n",
    "            observation, reward, done, state  = env.step(action)\n",
    "            out.write(state)\n",
    "            episode_reward += reward\n",
    "            t += 1\n",
    "            if done:\n",
    "                print(\"Episode {} finished after {} timesteps\".format(episode+1, t+1))\n",
    "                break\n",
    "        total_reward.append(episode_reward)\n",
    "    env.stop()\n",
    "    out.release()  \n",
    "    cv2.destroyAllWindows()\n",
    "    return np.mean(total_reward) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = Game(True)\n",
    "test_agent(env, transform, agent=model, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}